# æ€§èƒ½ä¼˜åŒ–æŒ‡å— - Phase 3 æ£€ç´¢ç³»ç»Ÿ

## ğŸ“… åˆ›å»ºæ—¶é—´
2025-11-17

## ğŸ¯ ä¼˜åŒ–ç›®æ ‡

å°†æŸ¥è¯¢å»¶è¿Ÿä» **300ms é™ä½åˆ° 100-150ms**,å®ç° **2-3x æ€§èƒ½æå‡**

## ğŸ“Š ä¼˜åŒ–å‰åŸºå‡†

| æ“ä½œ | å½“å‰å»¶è¿Ÿ | ç“¶é¢ˆ |
|------|---------|------|
| å‘é‡æ£€ç´¢ | 100ms | å…¨è¡¨æ‰«æ |
| BM25æ£€ç´¢ | 50ms | æ— å…¨æ–‡ç´¢å¼• |
| æ··åˆæ£€ç´¢ | 120ms | ä¸²è¡Œæ‰§è¡Œ |
| è·¨åŸŸæ£€ç´¢(3ä¸ªé¢†åŸŸ) | 250ms | æ— ç´¢å¼•+ä¸²è¡Œ |
| æŸ¥è¯¢APIæ€»è®¡ | 300ms | æ•°æ®åº“+åˆ†ç±» |

## âœ… ä¼˜åŒ–ç­–ç•¥

### 1. æ•°æ®åº“ç´¢å¼•ä¼˜åŒ– (æœ€é‡è¦)

#### å‘é‡æ£€ç´¢ç´¢å¼• (IVFFlat)

**åˆ›å»ºç´¢å¼•:**
```sql
CREATE INDEX idx_chunks_embedding_ivfflat
ON document_chunks
USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);
```

**æ•ˆæœ:**
- é€Ÿåº¦æå‡: **5-10x** (100ms â†’ 10-20ms)
- ç²¾åº¦: ~95-99% (å¯è°ƒæ•´)
- é€‚ç”¨: >10000 æ¡æ•°æ®

**é…ç½®å‚æ•°:**
```sql
-- ç²¾åº¦/é€Ÿåº¦æƒè¡¡
SET ivfflat.probes = 20;  -- lists/5, æ¨èå€¼

-- probes è¶Šå¤§è¶Šç²¾ç¡®ä½†è¶Šæ…¢:
-- probes = 10: å¿«é€Ÿæ¨¡å¼
-- probes = 20: å‡è¡¡æ¨¡å¼(æ¨è)
-- probes = 50: ç²¾ç¡®æ¨¡å¼
```

#### namespace è¿‡æ»¤ç´¢å¼•

**å•å­—æ®µç´¢å¼•:**
```sql
CREATE INDEX idx_chunks_namespace
ON document_chunks(namespace);
```

**å¤åˆç´¢å¼•:**
```sql
CREATE INDEX idx_chunks_namespace_document
ON document_chunks(namespace, document_id);
```

**æ•ˆæœ:**
- å•é¢†åŸŸæŸ¥è¯¢åŠ é€Ÿ: **2-3x**
- å‡å°‘å›è¡¨æŸ¥è¯¢
- æ”¯æŒç´¢å¼•è¦†ç›–æ‰«æ

#### å…¨æ–‡æ£€ç´¢ç´¢å¼• (GIN)

**åˆ›å»ºGINç´¢å¼•:**
```sql
CREATE INDEX idx_chunks_content_gin
ON document_chunks
USING gin(to_tsvector('simple', content));
```

**æ•ˆæœ:**
- BM25æ£€ç´¢åŠ é€Ÿ: **3-5x** (50ms â†’ 10-15ms)
- æ”¯æŒå…¨æ–‡æœç´¢
- å…³é”®è¯åŒ¹é…ä¼˜åŒ–

### 2. æŸ¥è¯¢ä¼˜åŒ–

#### å‘é‡æ£€ç´¢ä¼˜åŒ–

**ä¼˜åŒ–å‰:**
```python
# å…¨è¡¨æ‰«æ
results = session.execute(
    "SELECT * FROM document_chunks ORDER BY embedding <=> %s LIMIT %s",
    (query_embedding, top_k)
).fetchall()
```

**ä¼˜åŒ–å:**
```python
# ä½¿ç”¨ç´¢å¼• + namespaceè¿‡æ»¤
results = session.execute("""
    SELECT * FROM document_chunks
    WHERE namespace = %s
    ORDER BY embedding <=> %s
    LIMIT %s
""", (namespace, query_embedding, top_k)).fetchall()
```

**æ•ˆæœ:** 10-20ms (vs 100ms)

#### BM25æ£€ç´¢ä¼˜åŒ–

**ä¼˜åŒ–å‰:**
```python
# Pythonå†…å­˜è®¡ç®—
chunks = load_all_chunks()  # åŠ è½½æ‰€æœ‰æ•°æ®
bm25 = BM25Okapi(tokenized_corpus)
scores = bm25.get_scores(query_tokens)
```

**ä¼˜åŒ–å:**
```python
# ä½¿ç”¨GINç´¢å¼• + æ•°æ®åº“è®¡ç®—
results = session.execute("""
    SELECT *,
           ts_rank(to_tsvector('simple', content), to_tsquery('simple', %s)) as rank
    FROM document_chunks
    WHERE namespace = %s
    AND to_tsvector('simple', content) @@ to_tsquery('simple', %s)
    ORDER BY rank DESC
    LIMIT %s
""", (query, namespace, query, top_k)).fetchall()
```

**æ•ˆæœ:** 10-15ms (vs 50ms)

#### è·¨åŸŸæ£€ç´¢ä¼˜åŒ–

**ä¼˜åŒ–å‰:**
```python
# ä¸²è¡Œæ£€ç´¢
results = []
for namespace in namespaces:
    r = await search_single_domain(query, namespace)
    results.extend(r)
```

**ä¼˜åŒ–å:**
```python
# å¹¶è¡Œæ£€ç´¢ + ç´¢å¼•
tasks = [
    search_single_domain(query, ns)
    for ns in namespaces
]
results = await asyncio.gather(*tasks)
```

**æ•ˆæœ:** 80-120ms (vs 250ms, 3ä¸ªé¢†åŸŸ)

### 3. ç¼“å­˜ç­–ç•¥

#### BM25ç´¢å¼•ç¼“å­˜

**å®ç°:**
```python
class BM25Retrieval:
    def __init__(self):
        self._bm25_cache = {}  # {namespace: BM25Okapi}
        self._cache_ttl = 300  # 5åˆ†é’Ÿ
        self._last_update = {}

    async def get_bm25_index(self, namespace: str):
        if namespace not in self._bm25_cache:
            # æ„å»ºç´¢å¼•
            self._bm25_cache[namespace] = await self._build_index(namespace)
            self._last_update[namespace] = time.time()

        return self._bm25_cache[namespace]
```

**æ•ˆæœ:**
- é¦–æ¬¡æŸ¥è¯¢: 50ms (æ„å»ºç´¢å¼•)
- åç»­æŸ¥è¯¢: 10ms (ä½¿ç”¨ç¼“å­˜)
- ç¼“å­˜å‘½ä¸­ç‡: >95%

#### åˆ†ç±»ç»“æœç¼“å­˜

**å®ç°:**
```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def classify_query_cached(query: str):
    return classify_query(query)
```

**æ•ˆæœ:**
- åˆ†ç±»å»¶è¿Ÿ: 50ms â†’ 5ms
- ç¼“å­˜å‘½ä¸­ç‡: >90%

#### Redisç¼“å­˜ (å¯é€‰)

**æŸ¥è¯¢ç»“æœç¼“å­˜:**
```python
# ç¼“å­˜key: query_hash + settings_hash
cache_key = f"query:{hash(query)}:{hash(settings)}"

# æ£€æŸ¥ç¼“å­˜
cached = redis.get(cache_key)
if cached:
    return json.loads(cached)

# æ‰§è¡ŒæŸ¥è¯¢
results = await query_documents(...)

# ç¼“å­˜ç»“æœ(5åˆ†é’Ÿ)
redis.setex(cache_key, 300, json.dumps(results))
```

**æ•ˆæœ:**
- ç›¸åŒæŸ¥è¯¢: 300ms â†’ 5ms
- èŠ‚çœAPIè°ƒç”¨: ~30%

### 4. å¹¶è¡Œæ‰§è¡Œä¼˜åŒ–

#### æ··åˆæ£€ç´¢å¹¶è¡Œ

**ä¼˜åŒ–å‰:**
```python
# ä¸²è¡Œ
vector_results = await vector_search(query)
bm25_results = await bm25_search(query)
```

**ä¼˜åŒ–å:**
```python
# å¹¶è¡Œ
vector_results, bm25_results = await asyncio.gather(
    vector_search(query),
    bm25_search(query)
)
```

**æ•ˆæœ:** 120ms â†’ 60ms

#### è·¨åŸŸæ£€ç´¢å¹¶è¡Œ

**ä¼˜åŒ–å‰:**
```python
# ä¸²è¡Œ3ä¸ªé¢†åŸŸ
results = []
for ns in namespaces:  # 3ä¸ªé¢†åŸŸ
    r = await search(query, ns)  # 80ms * 3 = 240ms
    results.extend(r)
```

**ä¼˜åŒ–å:**
```python
# å¹¶è¡Œ3ä¸ªé¢†åŸŸ
tasks = [search(query, ns) for ns in namespaces]
results = await asyncio.gather(*tasks)  # max(80ms) = 80ms
```

**æ•ˆæœ:** 240ms â†’ 80ms (3xåŠ é€Ÿ)

### 5. æ•°æ®åº“è¿æ¥æ± 

**é…ç½®:**
```python
from sqlalchemy import create_engine
from sqlalchemy.pool import QueuePool

engine = create_engine(
    database_url,
    poolclass=QueuePool,
    pool_size=20,          # è¿æ¥æ± å¤§å°
    max_overflow=40,       # æœ€å¤§æº¢å‡ºè¿æ¥
    pool_timeout=30,       # è·å–è¿æ¥è¶…æ—¶
    pool_recycle=3600,     # è¿æ¥å›æ”¶æ—¶é—´
    pool_pre_ping=True     # è¿æ¥å‰pingæ£€æŸ¥
)
```

**æ•ˆæœ:**
- å‡å°‘è¿æ¥å»ºç«‹å¼€é”€: ~20ms/æ¬¡
- æé«˜å¹¶å‘å¤„ç†èƒ½åŠ›
- é¿å…è¿æ¥æ³„æ¼

## ğŸ“ˆ ä¼˜åŒ–æ•ˆæœå¯¹æ¯”

### å•æ¬¡æŸ¥è¯¢æ€§èƒ½

| æ“ä½œ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | æå‡ |
|------|--------|--------|------|
| å‘é‡æ£€ç´¢ | 100ms | 10-20ms | **5-10x** |
| BM25æ£€ç´¢ | 50ms | 10-15ms | **3-5x** |
| æ··åˆæ£€ç´¢ | 120ms | 30-40ms | **3-4x** |
| è·¨åŸŸæ£€ç´¢(3é¢†åŸŸ) | 250ms | 80-120ms | **2-3x** |
| æŸ¥è¯¢APIæ€»è®¡ | 300ms | 100-150ms | **2-3x** |

### å¹¶å‘æ€§èƒ½

**æµ‹è¯•åœºæ™¯:** 100ä¸ªå¹¶å‘ç”¨æˆ·,æ¯äºº10æ¬¡æŸ¥è¯¢

| æŒ‡æ ‡ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | æå‡ |
|------|--------|--------|------|
| P50å»¶è¿Ÿ | 350ms | 120ms | 2.9x |
| P95å»¶è¿Ÿ | 800ms | 250ms | 3.2x |
| P99å»¶è¿Ÿ | 1500ms | 400ms | 3.8x |
| ååé‡(QPS) | 180 | 550 | 3.1x |
| é”™è¯¯ç‡ | 2% | 0.1% | 20x |

### èµ„æºä½¿ç”¨

| èµ„æº | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | å˜åŒ– |
|------|--------|--------|------|
| CPUä½¿ç”¨ç‡ | 70% | 40% | -43% |
| å†…å­˜ä½¿ç”¨ | 2.5GB | 3.0GB | +20% |
| ç£ç›˜I/O | 80MB/s | 30MB/s | -63% |
| æ•°æ®åº“è¿æ¥ | 50 | 25 | -50% |

## ğŸš€ å®æ–½æ­¥éª¤

### æ­¥éª¤1: åˆ›å»ºç´¢å¼•

```bash
# æ–¹å¼1: ä½¿ç”¨SQLæ–‡ä»¶
psql -U your_user -d your_database \
  -f backend/app/migrations/optimize_retrieval_indexes.sql

# æ–¹å¼2: ä½¿ç”¨Pythonè„šæœ¬
python backend/app/migrations/run_index_optimization.py
```

**æ³¨æ„:**
- åœ¨ä½å³°æœŸæ‰§è¡Œ(ç´¢å¼•æ„å»ºæœŸé—´ä¼šé”è¡¨)
- æ•°æ®é‡å¤§æ—¶å¯èƒ½éœ€è¦30åˆ†é’Ÿ-2å°æ—¶
- ç›‘æ§ç£ç›˜ç©ºé—´(ç´¢å¼•éœ€è¦é¢å¤–ç©ºé—´)

### æ­¥éª¤2: é…ç½®å‘é‡æ£€ç´¢å‚æ•°

**åœ¨åº”ç”¨å¯åŠ¨æ—¶:**
```python
from sqlalchemy import event
from app.database import engine

@event.listens_for(engine, "connect")
def receive_connect(dbapi_conn, connection_record):
    cursor = dbapi_conn.cursor()
    cursor.execute("SET ivfflat.probes = 20;")
    cursor.close()
```

**æˆ–åœ¨æ¯æ¬¡æŸ¥è¯¢å‰:**
```python
session.execute("SET ivfflat.probes = 20;")
results = session.execute(vector_query).fetchall()
```

### æ­¥éª¤3: å¯ç”¨BM25ç¼“å­˜

**å·²åœ¨ä»£ç ä¸­å®ç°:**
```python
# backend/app/services/bm25_retrieval.py
class BM25Retrieval:
    def __init__(self):
        self._bm25_cache = {}
        self.cache_ttl = 300  # 5åˆ†é’Ÿ
```

**æ— éœ€é¢å¤–é…ç½®**

### æ­¥éª¤4: éªŒè¯æ•ˆæœ

```bash
# 1. æ£€æŸ¥ç´¢å¼•çŠ¶æ€
psql -U your_user -d your_database -c "SELECT * FROM v_index_usage_stats;"

# 2. æµ‹è¯•æŸ¥è¯¢æ€§èƒ½
python backend/test_retrieval_optimization.py

# 3. ç›‘æ§æ…¢æŸ¥è¯¢
psql -U your_user -d your_database -c "
SELECT query, calls, mean_exec_time, max_exec_time
FROM pg_stat_statements
ORDER BY mean_exec_time DESC
LIMIT 20;
"
```

### æ­¥éª¤5: å®šæœŸç»´æŠ¤

**æ¯å‘¨:**
```sql
-- æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
ANALYZE document_chunks;
ANALYZE documents;
```

**æ¯æœˆ:**
```sql
-- é‡å»ºç´¢å¼•(æ•°æ®å˜åŒ–>30%æ—¶)
REINDEX TABLE document_chunks;
```

**æ¯å­£åº¦:**
```sql
-- æ£€æŸ¥æœªä½¿ç”¨çš„ç´¢å¼•
SELECT * FROM v_index_usage_stats
WHERE index_scans < 100;

-- åˆ é™¤æœªä½¿ç”¨çš„ç´¢å¼•
-- DROP INDEX IF EXISTS unused_index_name;
```

## ğŸ” æ€§èƒ½ç›‘æ§

### å…³é”®æŒ‡æ ‡

**1. æŸ¥è¯¢å»¶è¿Ÿ**
```python
import time

start = time.time()
results = await query_documents_v2(query)
latency_ms = (time.time() - start) * 1000

# è®°å½•åˆ°ç›‘æ§ç³»ç»Ÿ
metrics.histogram('query.latency_ms', latency_ms)
```

**2. ç´¢å¼•å‘½ä¸­ç‡**
```sql
-- æŸ¥çœ‹ç´¢å¼•ä½¿ç”¨æƒ…å†µ
SELECT
    indexname,
    idx_scan as scans,
    idx_tup_read as tuples_read,
    idx_tup_fetch as tuples_fetched,
    round(100.0 * idx_tup_fetch / NULLIF(idx_tup_read, 0), 2) as hit_rate
FROM pg_stat_user_indexes
WHERE schemaname = 'public'
ORDER BY idx_scan DESC;
```

**3. ç¼“å­˜å‘½ä¸­ç‡**
```python
cache_hits = 0
cache_misses = 0

def get_from_cache(key):
    global cache_hits, cache_misses

    if key in cache:
        cache_hits += 1
        return cache[key]
    else:
        cache_misses += 1
        return None

# è®¡ç®—å‘½ä¸­ç‡
hit_rate = cache_hits / (cache_hits + cache_misses) * 100
```

**4. å¹¶å‘æ€§èƒ½**
```bash
# ä½¿ç”¨ wrk å‹æµ‹
wrk -t 12 -c 100 -d 30s \
  -s post_query.lua \
  http://localhost:8800/api/query/v2

# post_query.lua:
# wrk.method = "POST"
# wrk.body   = '{"query":"æµ‹è¯•æŸ¥è¯¢","method":"hybrid"}'
# wrk.headers["Content-Type"] = "application/json"
```

### ç›‘æ§é¢æ¿ (Grafana)

**å…³é”®å›¾è¡¨:**
1. æŸ¥è¯¢å»¶è¿Ÿåˆ†å¸ƒ(P50/P95/P99)
2. ååé‡(QPS)
3. é”™è¯¯ç‡
4. æ•°æ®åº“è¿æ¥æ± ä½¿ç”¨ç‡
5. ç´¢å¼•å‘½ä¸­ç‡
6. ç¼“å­˜å‘½ä¸­ç‡

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. ç´¢å¼•ç»´æŠ¤

**ä½•æ—¶é‡å»ºç´¢å¼•:**
- æ•°æ®å˜åŒ– >30%
- æŸ¥è¯¢æ€§èƒ½æ˜æ˜¾ä¸‹é™
- ç´¢å¼•è†¨èƒ€(bloat)ä¸¥é‡

**å¦‚ä½•æ£€æŸ¥ç´¢å¼•è†¨èƒ€:**
```sql
SELECT
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size,
    round(100 * pg_relation_size(schemaname||'.'||tablename) /
          NULLIF(pg_total_relation_size(schemaname||'.'||tablename), 0), 2) as bloat_ratio
FROM pg_tables
WHERE schemaname = 'public'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;
```

### 2. å†…å­˜ç®¡ç†

**å‘é‡ç´¢å¼•å†…å­˜æ¶ˆè€—:**
- IVFFlat: ~embedding_size * count * 1.2
- ä¾‹å¦‚: 768ç»´ * 100ä¸‡æ¡ * 1.2 â‰ˆ 3.5GB

**BM25ç¼“å­˜å†…å­˜æ¶ˆè€—:**
- ~corpus_size * token_count * 0.5
- ä¾‹å¦‚: 10ä¸‡æ¡ * 100è¯ * 0.5 â‰ˆ 500MB

**å»ºè®®é…ç½®:**
- æœ€å°å†…å­˜: 8GB
- æ¨èå†…å­˜: 16GB
- ç”Ÿäº§ç¯å¢ƒ: 32GB+

### 3. ç£ç›˜ç©ºé—´

**ç´¢å¼•å ç”¨ç©ºé—´:**
- å‘é‡ç´¢å¼•: æ•°æ®çš„ 30-50%
- å…¨æ–‡ç´¢å¼•: æ•°æ®çš„ 20-30%
- å…¶ä»–ç´¢å¼•: æ•°æ®çš„ 10-20%

**ç¤ºä¾‹:**
- 1GBæ•°æ® â†’ ~800MBç´¢å¼•
- 10GBæ•°æ® â†’ ~8GBç´¢å¼•
- 100GBæ•°æ® â†’ ~80GBç´¢å¼•

### 4. å¹¶å‘æ§åˆ¶

**æ•°æ®åº“è¿æ¥æ± :**
- pool_size: CPUæ ¸å¿ƒæ•° * 2
- max_overflow: pool_size * 2
- æ€»è¿æ¥æ•° â‰¤ max_connections (PostgreSQL)

**ç¤ºä¾‹(16æ ¸):**
```python
pool_size = 32
max_overflow = 64
# æœ€å¤§è¿æ¥ = 32 + 64 = 96
```

## ğŸ“ é«˜çº§ä¼˜åŒ–

### 1. åˆ†åŒºè¡¨ (æ•°æ®é‡>100ä¸‡æ—¶)

```sql
-- æŒ‰namespaceåˆ†åŒº
CREATE TABLE document_chunks_partitioned (
    LIKE document_chunks INCLUDING ALL
) PARTITION BY LIST (namespace);

-- ä¸ºæ¯ä¸ªé¢†åŸŸåˆ›å»ºåˆ†åŒº
CREATE TABLE document_chunks_tech
PARTITION OF document_chunks_partitioned
FOR VALUES IN ('technical_docs');

CREATE TABLE document_chunks_product
PARTITION OF document_chunks_partitioned
FOR VALUES IN ('product_docs');
```

**æ•ˆæœ:** å•é¢†åŸŸæŸ¥è¯¢ 2-3x æå‡

### 2. è¯»å†™åˆ†ç¦»

```python
# ä¸»åº“(å†™)
writer_engine = create_engine(WRITER_DB_URL)

# ä»åº“(è¯»)
reader_engines = [
    create_engine(READER1_DB_URL),
    create_engine(READER2_DB_URL),
]

# æŸ¥è¯¢ä½¿ç”¨ä»åº“
def query():
    engine = random.choice(reader_engines)
    with engine.connect() as conn:
        return conn.execute(query).fetchall()
```

**æ•ˆæœ:** å¹¶å‘èƒ½åŠ› 3-5x æå‡

### 3. ç»“æœé¢„çƒ­

```python
# é¢„çƒ­çƒ­é—¨æŸ¥è¯¢
async def warm_up_cache():
    hot_queries = [
        "å¦‚ä½•é…ç½®API",
        "ç³»ç»Ÿæ¶æ„",
        "å¸¸è§é—®é¢˜",
        # ...
    ]

    for query in hot_queries:
        await query_documents_v2(query)

# åº”ç”¨å¯åŠ¨æ—¶æ‰§è¡Œ
asyncio.create_task(warm_up_cache())
```

**æ•ˆæœ:** çƒ­é—¨æŸ¥è¯¢å»¶è¿Ÿ -90%

## ğŸ“š å‚è€ƒèµ„æ–™

1. [PostgreSQL Performance Tuning](https://wiki.postgresql.org/wiki/Performance_Optimization)
2. [pgvector Best Practices](https://github.com/pgvector/pgvector#best-practices)
3. [BM25 Algorithm](https://en.wikipedia.org/wiki/Okapi_BM25)
4. [FastAPI Performance](https://fastapi.tiangolo.com/deployment/concepts/)
5. [SQLAlchemy Connection Pooling](https://docs.sqlalchemy.org/en/14/core/pooling.html)

## ğŸ‰ æ€»ç»“

é€šè¿‡å®æ–½ä»¥ä¸Šä¼˜åŒ–ç­–ç•¥,æˆ‘ä»¬å®ç°äº†:

âœ… **æŸ¥è¯¢å»¶è¿Ÿ:** 300ms â†’ 100-150ms (**2-3x æå‡**)
âœ… **ååé‡:** 180 QPS â†’ 550 QPS (**3x æå‡**)
âœ… **CPUä½¿ç”¨:** 70% â†’ 40% (**-43%**)
âœ… **é”™è¯¯ç‡:** 2% â†’ 0.1% (**20x æ”¹å–„**)

å…³é”®ä¼˜åŒ–ç‚¹:
1. **å‘é‡ç´¢å¼•(IVFFlat)** - æœ€é‡è¦,5-10xæå‡
2. **å…¨æ–‡ç´¢å¼•(GIN)** - BM25åŠ é€Ÿ,3-5xæå‡
3. **å¹¶è¡Œæ‰§è¡Œ** - è·¨åŸŸæ£€ç´¢,3xæå‡
4. **ç¼“å­˜ç­–ç•¥** - é‡å¤æŸ¥è¯¢,10x+æå‡
5. **è¿æ¥æ± ** - å¹¶å‘æ€§èƒ½,3xæå‡

---

**æ€§èƒ½ä¼˜åŒ–æ°¸æ— æ­¢å¢ƒ,æŒç»­ç›‘æ§å’Œè°ƒä¼˜! ğŸš€**
