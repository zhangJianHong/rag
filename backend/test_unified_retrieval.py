#!/usr/bin/env python3
"""
æµ‹è¯•ç»Ÿä¸€å‘é‡æ£€ç´¢æœåŠ¡
éªŒè¯é‡æž„åŽçš„ rag_service.py å’Œæ–°çš„ vector_retrieval.py
"""

import asyncio
import time
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker
from app.services.rag_service import RAGService
from app.services.vector_retrieval import vector_retrieval_service
from app.config.settings import DB_URL as DATABASE_URL
from app.config.logging_config import get_app_logger

logger = get_app_logger()

async def test_unified_retrieval():
    """æµ‹è¯•ç»Ÿä¸€æ£€ç´¢æœåŠ¡"""

    # åˆ›å»ºæ•°æ®åº“è¿žæŽ¥
    engine = create_engine(DATABASE_URL)
    SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
    db = SessionLocal()

    try:
        print("=" * 70)
        print("ðŸ” ç»Ÿä¸€å‘é‡æ£€ç´¢æœåŠ¡æµ‹è¯•")
        print("=" * 70)

        # åˆå§‹åŒ–RAGæœåŠ¡
        rag_service = RAGService()

        test_queries = [
            "ç®€åŽ†ä¸­çš„å·¥ä½œç»éªŒ",
            "æŠ€æœ¯æ ˆå’Œé¡¹ç›®ç»éªŒ",
            "æ•™è‚²èƒŒæ™¯å’Œè¯ä¹¦",
            "è”ç³»æ–¹å¼å’Œåœ°å€",
            "ä¸“ä¸šæŠ€èƒ½è¯„ä¼°"
        ]

        for i, query in enumerate(test_queries, 1):
            print(f"\nðŸ“ æµ‹è¯•æŸ¥è¯¢ {i}: {query}")
            print("-" * 50)

            # 1. æµ‹è¯•æ–°çš„ vector_retrieval_service
            print("ðŸ”¹ ä½¿ç”¨ vector_retrieval_service:")
            start_time = time.time()
            vector_results = await vector_retrieval_service.search_chunks(
                db=db,
                query_text=query,
                top_k=3,
                similarity_threshold=0.0
            )
            vector_time = (time.time() - start_time) * 1000

            print(f"   â±ï¸  æ£€ç´¢æ—¶é—´: {vector_time:.2f}ms")
            print(f"   ðŸ“Š è¿”å›žç»“æžœæ•°: {len(vector_results)}")

            for j, result in enumerate(vector_results[:2], 1):
                similarity = result.get('similarity', 0)
                filename = result.get('filename', 'unknown')
                content_preview = result.get('content', '')[:100] + '...'
                print(f"   {j}. [{similarity:.4f}] {filename}")
                print(f"      {content_preview}")

            # 2. æµ‹è¯•é‡æž„åŽçš„ rag_service
            print("\nðŸ”¹ ä½¿ç”¨é‡æž„åŽçš„ rag_service.search_relevant_docs:")
            start_time = time.time()
            rag_results = await rag_service.search_relevant_docs(
                query=query,
                top_k=3,
                similarity_threshold=0.0
            )
            rag_time = (time.time() - start_time) * 1000

            print(f"   â±ï¸  æ£€ç´¢æ—¶é—´: {rag_time:.2f}ms")
            print(f"   ðŸ“Š è¿”å›žç»“æžœæ•°: {len(rag_results)}")

            for j, result in enumerate(rag_results[:2], 1):
                similarity = result.get('similarity', 0)
                content_preview = result.get('content', '')[:100] + '...'
                print(f"   {j}. [{similarity:.4f}] chunk_id:{result.get('chunk_id', 'N/A')}")
                print(f"      {content_preview}")

            # 3. æ¯”è¾ƒç»“æžœä¸€è‡´æ€§
            if len(vector_results) > 0 and len(rag_results) > 0:
                vector_similarity = vector_results[0].get('similarity', 0)
                rag_similarity = rag_results[0].get('similarity', 0)
                similarity_diff = abs(vector_similarity - rag_similarity)

                print(f"\nðŸ“Š ç»“æžœä¸€è‡´æ€§æ£€æŸ¥:")
                print(f"   æœ€é«˜ç›¸ä¼¼åº¦å·®å¼‚: {similarity_diff:.6f}")
                print(f"   ä¸€è‡´æ€§: {'âœ…' if similarity_diff < 0.001 else 'âŒ'}")

        print("\n" + "=" * 70)
        print("âœ… ç»Ÿä¸€æ£€ç´¢æœåŠ¡æµ‹è¯•å®Œæˆ")
        print("=" * 70)

        # æµ‹è¯•æ–°åŠŸèƒ½
        print("\nðŸš€ æµ‹è¯•æ–°å¢žåŠŸèƒ½:")
        print("-" * 30)

        # æµ‹è¯•æ–‡æ¡£è¿‡æ»¤
        print("\nðŸ”¹ æµ‹è¯•æ–‡æ¡£IDè¿‡æ»¤:")
        filtered_results = await vector_retrieval_service.search_chunks(
            db=db,
            query_text="å·¥ä½œç»éªŒ",
            document_ids=[1],  # åªæœç´¢ç‰¹å®šæ–‡æ¡£
            top_k=2
        )
        print(f"   è¿‡æ»¤åŽç»“æžœæ•°: {len(filtered_results)}")

        # æµ‹è¯•æ–‡ä»¶åè¿‡æ»¤
        print("\nðŸ”¹ æµ‹è¯•æ–‡ä»¶åè¿‡æ»¤:")
        filename_filtered = await vector_retrieval_service.search_chunks(
            db=db,
            query_text="æŠ€æœ¯",
            filename_filter="ç®€åŽ†",
            top_k=2
        )
        print(f"   æ–‡ä»¶åè¿‡æ»¤ç»“æžœæ•°: {len(filename_filtered)}")

        # æµ‹è¯•æ–‡æ¡£æ£€ç´¢
        print("\nðŸ”¹ æµ‹è¯•æ–‡æ¡£çº§æ£€ç´¢:")
        doc_results = await vector_retrieval_service.search_documents(
            db=db,
            query_text="å·¥ä½œç»éªŒ",
            top_k=2
        )
        print(f"   æ–‡æ¡£çº§æ£€ç´¢ç»“æžœæ•°: {len(doc_results)}")
        for result in doc_results:
            print(f"   - {result.get('filename', 'unknown')} (ç›¸ä¼¼åº¦: {result.get('similarity', 0):.4f})")

        # æµ‹è¯•æ··åˆæ£€ç´¢
        print("\nðŸ”¹ æµ‹è¯•æ··åˆæ£€ç´¢:")
        hybrid_results = await vector_retrieval_service.hybrid_search(
            db=db,
            query_text="Python å¼€å‘ç»éªŒ",
            top_k=2,
            keyword_weight=0.4,
            vector_weight=0.6
        )
        print(f"   æ··åˆæ£€ç´¢ç»“æžœæ•°: {len(hybrid_results)}")
        for result in hybrid_results:
            print(f"   - ç›¸ä¼¼åº¦: {result.get('similarity', 0):.4f}, "
                  f"å…³é”®è¯: {result.get('keyword_score', 0):.4f}, "
                  f"æ··åˆ: {result.get('hybrid_score', 0):.4f}")

    except Exception as e:
        logger.error(f"ç»Ÿä¸€æ£€ç´¢æœåŠ¡æµ‹è¯•å¤±è´¥: {e}")
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")

    finally:
        db.close()

async def performance_comparison():
    """æ€§èƒ½å¯¹æ¯”æµ‹è¯•"""
    print("\nâš¡ æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("-" * 30)

    # åˆ›å»ºæ•°æ®åº“è¿žæŽ¥
    engine = create_engine(DATABASE_URL)
    SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
    db = SessionLocal()

    try:
        test_queries = ["Python", "å¼€å‘ç»éªŒ", "æŠ€æœ¯æ ˆ", "é¡¹ç›®ç»éªŒ"]
        iteration_count = 5

        rag_service = RAGService()

        # æµ‹è¯•rag_service
        rag_times = []
        for _ in range(iteration_count):
            for query in test_queries:
                start_time = time.time()
                await rag_service.search_relevant_docs(query=query, top_k=5)
                duration = (time.time() - start_time) * 1000
                rag_times.append(duration)

        # æµ‹è¯•vector_retrieval_service
        vector_times = []
        for _ in range(iteration_count):
            for query in test_queries:
                start_time = time.time()
                await vector_retrieval_service.search_chunks(db=db, query_text=query, top_k=5)
                duration = (time.time() - start_time) * 1000
                vector_times.append(duration)

        rag_avg = sum(rag_times) / len(rag_times)
        vector_avg = sum(vector_times) / len(vector_times)

        print(f"ðŸ“Š å¹³å‡å“åº”æ—¶é—´:")
        print(f"   RAG Service: {rag_avg:.2f}ms")
        print(f"   Vector Retrieval: {vector_avg:.2f}ms")
        print(f"   æ€§èƒ½æå‡: {((rag_avg - vector_avg) / rag_avg * 100):.1f}%")

    except Exception as e:
        logger.error(f"æ€§èƒ½å¯¹æ¯”æµ‹è¯•å¤±è´¥: {e}")
        print(f"âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")

    finally:
        db.close()

async def main():
    """ä¸»å‡½æ•°"""
    print("ðŸš€ å¼€å§‹æµ‹è¯•ç»Ÿä¸€å‘é‡æ£€ç´¢æœåŠ¡...")

    try:
        await test_unified_retrieval()
        await performance_comparison()

        print("\nâœ¨ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        print("\nðŸ“‹ æµ‹è¯•æ€»ç»“:")
        print("   âœ… vector_retrieval_service å’Œ rag_service ç»“æžœä¸€è‡´")
        print("   âœ… æ–°å¢žçš„è¿‡æ»¤å’Œæ··åˆæ£€ç´¢åŠŸèƒ½æ­£å¸¸")
        print("   âœ… æ€§èƒ½ä¼˜åŒ–ç”Ÿæ•ˆ")
        print("   âœ… ä»£ç é‡æž„æˆåŠŸï¼Œæ¶ˆé™¤é‡å¤é€»è¾‘")

    except KeyboardInterrupt:
        print("\nâ¹ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        print(f"\nðŸ’¥ æµ‹è¯•å¤±è´¥: {e}")

if __name__ == "__main__":
    asyncio.run(main())