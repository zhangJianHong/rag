"""
æ•°æ®æ¸…ç†è„šæœ¬: æ¸…ç†é‡å¤æ–‡æ¡£
Phase 3: æ•°æ®åº“ä¼˜åŒ– - å‡†å¤‡å·¥ä½œ

åŠŸèƒ½:
1. è¯†åˆ«é‡å¤æ–‡æ¡£ (ç›¸åŒ filename + namespace)
2. ä¿ç•™æœ€æ–°ç‰ˆæœ¬(æ ¹æ® created_at æˆ– id)
3. åˆ é™¤æ—§ç‰ˆæœ¬åŠå…¶å…³è”çš„ document_chunks
4. ç”Ÿæˆæ¸…ç†æŠ¥å‘Š

æ‰§è¡Œæ–¹å¼:
1. é¢„è§ˆæ¨¡å¼ (é»˜è®¤): åªæ˜¾ç¤ºå°†è¦åˆ é™¤çš„æ•°æ®,ä¸å®é™…åˆ é™¤
   python cleanup_duplicate_documents.py

2. æ‰§è¡Œæ¨¡å¼: å®é™…åˆ é™¤é‡å¤æ•°æ®
   python cleanup_duplicate_documents.py --execute

3. å¼ºåˆ¶æ¨¡å¼: è·³è¿‡ç¡®è®¤ç›´æ¥åˆ é™¤
   python cleanup_duplicate_documents.py --execute --force
"""

import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
script_dir = Path(__file__).resolve().parent
backend_dir = script_dir.parent.parent  # backend/
sys.path.insert(0, str(backend_dir))

from sqlalchemy import create_engine, text
from app.config.settings import DB_URL
import logging
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def find_duplicates(conn):
    """æŸ¥æ‰¾æ‰€æœ‰é‡å¤çš„æ–‡æ¡£ç»„"""
    logger.info("æ­£åœ¨æ‰«æé‡å¤æ–‡æ¡£...")

    # æŸ¥æ‰¾é‡å¤çš„ (filename, namespace) ç»„åˆ
    result = conn.execute(text("""
        SELECT
            filename,
            namespace,
            COUNT(*) as duplicate_count,
            array_agg(id ORDER BY created_at DESC, id DESC) as doc_ids,
            array_agg(created_at ORDER BY created_at DESC, id DESC) as created_times
        FROM documents
        GROUP BY filename, namespace
        HAVING COUNT(*) > 1
        ORDER BY duplicate_count DESC, filename
    """))

    duplicates = result.fetchall()
    return duplicates


def get_document_stats(conn, doc_id):
    """è·å–æ–‡æ¡£çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"""
    # è·å–æ–‡æ¡£å—æ•°é‡
    chunks_result = conn.execute(text("""
        SELECT COUNT(*) FROM document_chunks WHERE document_id = :doc_id
    """), {"doc_id": doc_id})
    chunks_count = chunks_result.scalar()

    # è·å–æ–‡æ¡£å…ƒæ•°æ®
    doc_result = conn.execute(text("""
        SELECT
            id,
            filename,
            namespace,
            created_at,
            LENGTH(content) as content_length,
            doc_metadata
        FROM documents
        WHERE id = :doc_id
    """), {"doc_id": doc_id})
    doc = doc_result.fetchone()

    return {
        "id": doc.id if doc else None,
        "filename": doc.filename if doc else None,
        "namespace": doc.namespace if doc else None,
        "created_at": doc.created_at if doc else None,
        "content_length": doc.content_length if doc else 0,
        "chunks_count": chunks_count,
        "metadata": doc.doc_metadata if doc else None
    }


def preview_cleanup(duplicates, conn):
    """é¢„è§ˆå°†è¦åˆ é™¤çš„æ•°æ®"""
    logger.info("\n" + "="*80)
    logger.info("é¢„è§ˆæ¨¡å¼: ä»¥ä¸‹æ˜¯å°†è¦åˆ é™¤çš„é‡å¤æ–‡æ¡£")
    logger.info("="*80 + "\n")

    total_docs_to_delete = 0
    total_chunks_to_delete = 0

    for idx, dup in enumerate(duplicates, 1):
        filename = dup.filename
        namespace = dup.namespace
        doc_ids = dup.doc_ids
        duplicate_count = dup.duplicate_count

        logger.info(f"\nã€é‡å¤ç»„ {idx}/{len(duplicates)}ã€‘")
        logger.info(f"  æ–‡ä»¶å: {filename}")
        logger.info(f"  å‘½åç©ºé—´: {namespace}")
        logger.info(f"  é‡å¤æ•°é‡: {duplicate_count}")
        logger.info(f"  ä¿ç•™: æœ€æ–°ç‰ˆæœ¬ (ID: {doc_ids[0]})")
        logger.info(f"  åˆ é™¤: {duplicate_count - 1} ä¸ªæ—§ç‰ˆæœ¬")
        logger.info("")

        # æ˜¾ç¤ºæ¯ä¸ªç‰ˆæœ¬çš„è¯¦ç»†ä¿¡æ¯
        for i, doc_id in enumerate(doc_ids):
            stats = get_document_stats(conn, doc_id)
            status = "âœ“ ä¿ç•™" if i == 0 else "âœ— åˆ é™¤"

            logger.info(f"    {status} ID={doc_id}")
            logger.info(f"         åˆ›å»ºæ—¶é—´: {stats['created_at']}")
            logger.info(f"         å†…å®¹å¤§å°: {stats['content_length']} å­—ç¬¦")
            logger.info(f"         æ–‡æ¡£å—æ•°: {stats['chunks_count']}")

            if i > 0:  # ç»Ÿè®¡å°†è¦åˆ é™¤çš„
                total_docs_to_delete += 1
                total_chunks_to_delete += stats['chunks_count']

        logger.info("")

    logger.info("="*80)
    logger.info("æ¸…ç†ç»Ÿè®¡é¢„è§ˆ:")
    logger.info(f"  å°†åˆ é™¤æ–‡æ¡£: {total_docs_to_delete} ä¸ª")
    logger.info(f"  å°†åˆ é™¤æ–‡æ¡£å—: {total_chunks_to_delete} ä¸ª")
    logger.info(f"  é‡Šæ”¾ç©ºé—´: é¢„ä¼° {(total_chunks_to_delete * 1000) // 1024} KB")
    logger.info("="*80 + "\n")

    return total_docs_to_delete, total_chunks_to_delete


def execute_cleanup(duplicates, conn, force=False):
    """æ‰§è¡Œæ¸…ç†æ“ä½œ"""
    total_docs_deleted = 0
    total_chunks_deleted = 0
    errors = []

    logger.info("\n" + "="*80)
    logger.info("å¼€å§‹æ‰§è¡Œæ¸…ç†...")
    logger.info("="*80 + "\n")

    for idx, dup in enumerate(duplicates, 1):
        filename = dup.filename
        namespace = dup.namespace
        doc_ids = dup.doc_ids  # å·²æŒ‰åˆ›å»ºæ—¶é—´å€’åºæ’åˆ—

        # ä¿ç•™ç¬¬ä¸€ä¸ª(æœ€æ–°çš„),åˆ é™¤å…¶ä½™çš„
        keep_id = doc_ids[0]
        delete_ids = doc_ids[1:]

        logger.info(f"å¤„ç†é‡å¤ç»„ {idx}/{len(duplicates)}: {filename} ({namespace})")
        logger.info(f"  ä¿ç•™ ID={keep_id}, åˆ é™¤ {len(delete_ids)} ä¸ªæ—§ç‰ˆæœ¬...")

        for doc_id in delete_ids:
            try:
                # è·å–ç»Ÿè®¡ä¿¡æ¯
                stats = get_document_stats(conn, doc_id)

                # åˆ é™¤æ–‡æ¡£å—
                chunks_result = conn.execute(text("""
                    DELETE FROM document_chunks
                    WHERE document_id = :doc_id
                """), {"doc_id": doc_id})
                chunks_deleted = chunks_result.rowcount

                # åˆ é™¤ç”¨æˆ·å…³è”
                conn.execute(text("""
                    DELETE FROM user_documents
                    WHERE document_id = :doc_id
                """), {"doc_id": doc_id})

                # åˆ é™¤ç´¢å¼•è®°å½•
                conn.execute(text("""
                    DELETE FROM document_index_records
                    WHERE doc_id = :doc_id
                """), {"doc_id": doc_id})

                # åˆ é™¤æ–‡æ¡£
                doc_result = conn.execute(text("""
                    DELETE FROM documents
                    WHERE id = :doc_id
                """), {"doc_id": doc_id})
                docs_deleted = doc_result.rowcount

                total_docs_deleted += docs_deleted
                total_chunks_deleted += chunks_deleted

                logger.info(f"    âœ“ å·²åˆ é™¤ ID={doc_id}: {chunks_deleted} ä¸ªæ–‡æ¡£å—")

            except Exception as e:
                error_msg = f"åˆ é™¤æ–‡æ¡£ {doc_id} å¤±è´¥: {e}"
                logger.error(f"    âœ— {error_msg}")
                errors.append(error_msg)

        logger.info("")

    logger.info("="*80)
    logger.info("æ¸…ç†å®Œæˆ!")
    logger.info(f"  æˆåŠŸåˆ é™¤æ–‡æ¡£: {total_docs_deleted} ä¸ª")
    logger.info(f"  æˆåŠŸåˆ é™¤æ–‡æ¡£å—: {total_chunks_deleted} ä¸ª")

    if errors:
        logger.warning(f"  é”™è¯¯æ•°é‡: {len(errors)}")
        logger.warning("  é”™è¯¯è¯¦æƒ…:")
        for error in errors[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ªé”™è¯¯
            logger.warning(f"    - {error}")
        if len(errors) > 10:
            logger.warning(f"    ... è¿˜æœ‰ {len(errors) - 10} ä¸ªé”™è¯¯")

    logger.info("="*80 + "\n")

    return total_docs_deleted, total_chunks_deleted, errors


def run_cleanup(execute=False, force=False):
    """è¿è¡Œæ¸…ç†æµç¨‹"""
    engine = create_engine(DB_URL)

    with engine.begin() as conn:
        try:
            # æŸ¥æ‰¾é‡å¤æ–‡æ¡£
            duplicates = find_duplicates(conn)

            if not duplicates:
                logger.info("\nâœ… æ²¡æœ‰å‘ç°é‡å¤æ–‡æ¡£,æ•°æ®åº“çŠ¶æ€è‰¯å¥½!\n")
                return True

            logger.info(f"\nå‘ç° {len(duplicates)} ç»„é‡å¤æ–‡æ¡£\n")

            if not execute:
                # é¢„è§ˆæ¨¡å¼
                preview_cleanup(duplicates, conn)
                logger.info("ğŸ’¡ æç¤º:")
                logger.info("   è¿™æ˜¯é¢„è§ˆæ¨¡å¼,æ²¡æœ‰å®é™…åˆ é™¤æ•°æ®")
                logger.info("   å¦‚éœ€æ‰§è¡Œæ¸…ç†,è¯·ä½¿ç”¨: python cleanup_duplicate_documents.py --execute")
                logger.info("")
                return True

            # æ‰§è¡Œæ¨¡å¼
            # å…ˆé¢„è§ˆ
            total_docs, total_chunks = preview_cleanup(duplicates, conn)

            # ç¡®è®¤
            if not force:
                logger.warning("âš ï¸  è­¦å‘Š: å³å°†åˆ é™¤ä¸Šè¿°æ•°æ®,æ­¤æ“ä½œä¸å¯æ’¤é”€!")
                confirm = input("\næ˜¯å¦ç»§ç»­? (è¾“å…¥ 'YES' ç¡®è®¤): ")
                if confirm != "YES":
                    logger.info("å·²å–æ¶ˆæ“ä½œ")
                    return False

            # æ‰§è¡Œæ¸…ç†
            docs_deleted, chunks_deleted, errors = execute_cleanup(duplicates, conn)

            if errors:
                logger.warning(f"\nâš ï¸  æ¸…ç†å®Œæˆ,ä½†æœ‰ {len(errors)} ä¸ªé”™è¯¯")
                return False
            else:
                logger.info("\nâœ… æ¸…ç†æˆåŠŸå®Œæˆ!")
                logger.info("\nä¸‹ä¸€æ­¥:")
                logger.info("   è¿è¡Œæ•°æ®åº“è¿ç§»æ·»åŠ å”¯ä¸€çº¦æŸ:")
                logger.info("   python backend/app/migrations/add_document_constraints.py")
                logger.info("")
                return True

        except Exception as e:
            logger.error(f"\nâŒ æ¸…ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="æ¸…ç†é‡å¤æ–‡æ¡£")
    parser.add_argument("--execute", action="store_true",
                       help="æ‰§è¡Œæ¸…ç†(é»˜è®¤ä¸ºé¢„è§ˆæ¨¡å¼)")
    parser.add_argument("--force", action="store_true",
                       help="å¼ºåˆ¶æ‰§è¡Œ,è·³è¿‡ç¡®è®¤æç¤º")

    args = parser.parse_args()

    success = run_cleanup(execute=args.execute, force=args.force)
    sys.exit(0 if success else 1)
